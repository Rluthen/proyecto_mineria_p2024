{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza del Dataset\n",
    "\n",
    "La primera actividad consiste en la limpieza del dataset, de manera que éste cumpla con los requerimientos del “Tidy Data”, es decir:\n",
    "\n",
    "1. Cada renglón o fila del dataset corresponde a una observación o individuo.\n",
    "2. Cada columna del dataset corresponde a una y sólo una variable.\n",
    "3. No se mezcla información de naturaleza diferente.\n",
    "\n",
    "En el caso del dataset entregado por el INE, estos principios, sobre todo el número 1, no se respetan ya que en nuestro caso, una observación/individuo es una (y sólo una) pregunta. El resultado de esta actividad es un dataset (almacenado en un archivo CSV) que contenga los siguientes campos:\n",
    "\n",
    "- ID del registro\n",
    "- Entidad de Origen\n",
    "- Edad\n",
    "- Género\n",
    "- Identificación con algún grupo en situación de discriminación\n",
    "- Tema de la pregunta\n",
    "- Texto de la pregunta\n",
    "\n",
    "Es decir, en los casos en los que una persona envió más de una pregunta, cada pregunta debe estar en su renglón con el resto de los datos de identificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m limpio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m limpio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menero\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m01\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfebrero\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarzo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m03\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabril\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m04\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmayo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m05\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjunio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m06\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjulio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m07\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magosto\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m08\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseptiembre\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m09\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moctubre\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoviembre\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiciembre\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m12\u001b[39m\u001b[38;5;124m'\u001b[39m}, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     71\u001b[0m limpio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(limpio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfecha\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm, \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mI:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m limpio_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpregunta_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnormalize_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimpio_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpregunta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m display(limpio_df)\n\u001b[0;32m     77\u001b[0m limpio_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Base_limpia.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mnormalize_document\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     18\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(doc)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# filter stopwords out of document\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mreplace_accented\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# re-create document from filtered tokens\u001b[39;00m\n\u001b[0;32m     22\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_tokens)\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36mreplace_accented\u001b[1;34m(match)\u001b[0m\n\u001b[0;32m      7\u001b[0m unaccented \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maeiounuAEIOUNU\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m trans_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(accented, unaccented)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(trans_table)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "import re\n",
    "\n",
    "def replace_accented(match):\n",
    "    accented = 'áéíóúñüÁÉÍÓÚÑÜ'\n",
    "    unaccented = 'aeiounuAEIOUNU'\n",
    "    trans_table = str.maketrans(accented, unaccented)\n",
    "    return match.translate(trans_table)\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('spanish')\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\sáéíóúñüÁÉÍÓÚÑÜ]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [replace_accented(token) for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "def repetido(s, char):\n",
    "    return s.count(char) > 1\n",
    "\n",
    "crudo_df = pd.read_excel('./Base_PreguntasFormulario-1erDebatePresidencial2024.xlsx')\n",
    "\n",
    "crudo_df['Edad:'] = crudo_df['Edad:'].str.replace(' años', '').astype(int)\n",
    "\n",
    "crudo_df.rename(columns = {\n",
    "        '¿Cuál es tu entidad de origen?': 'entidad', \n",
    "        'Edad:': 'edad',\n",
    "        'Género:': 'genero',\n",
    "        '¿Te identificas con alguno de los siguientes Grupos en Situación de Discriminación?': 'grupo_discriminacion',\n",
    "        'Indica el tema de tu pregunta:': 'tema',\n",
    "        'Escribe tu pregunta:': 'pregunta',\n",
    "        '¿Deseas agregar otra pregunta?': 'extra',\n",
    "        'Indica el tema de tu pregunta: (2)': 'tema2',\n",
    "        'Escribe tu pregunta: (2)': 'pregunta2',\n",
    "        '¿Deseas agregar otra pregunta? (2)': 'extra2',\n",
    "        'Indica el tema de tu pregunta: (3)': 'tema3',\n",
    "        'Escribe tu pregunta: (3)': 'pregunta3',\n",
    "        '¿Deseas agregar otra pregunta? (3)': 'extra3',\n",
    "        'Indica el tema de tu pregunta: (4)': 'tema4',\n",
    "        'Escribe tu pregunta: (4)': 'pregunta4',\n",
    "        '¿Deseas agregar otra pregunta? (4)': 'extra4',\n",
    "        'Indica el tema de tu pregunta: (5)': 'tema5',\n",
    "        'Escribe tu pregunta: (5)': 'pregunta5',\n",
    "        '¿Deseas agregar otra pregunta? (5)': 'extra5',\n",
    "        'Indica el tema de tu pregunta: (6)': 'tema6',\n",
    "        'Escribe tu pregunta: (6)': 'pregunta6',\n",
    "        'Fecha de entrada': 'fecha'\n",
    "    }, inplace = True)\n",
    "\n",
    "todo_preguntas = []\n",
    "todo_preguntas.append(crudo_df[['entidad', 'edad', 'genero', 'grupo_discriminacion', 'tema','pregunta', 'fecha']])\n",
    "\n",
    "#Asignación de los Archivos a la Lista\n",
    "for idx in range(2, 6+1):\n",
    "    todo_preguntas.append(crudo_df[['entidad', 'edad', 'genero', 'grupo_discriminacion', 'tema'+str(idx),'pregunta'+str(idx), 'fecha']].rename(columns={'tema'+str(idx): 'tema', 'pregunta'+str(idx): 'pregunta'}))\n",
    "    \n",
    "#Concatenación de los Elementos de la Lista a un DataFrame\n",
    "limpio_df = pd.concat(todo_preguntas)\n",
    "limpio_df = limpio_df.dropna()\n",
    "limpio_df.drop_duplicates(inplace=True)\n",
    " \n",
    "limpio_df['grupo_discriminacion'] = limpio_df['grupo_discriminacion'].str.replace('Selecciona','N/A')\n",
    "limpio_df['fecha'] = limpio_df['fecha'].replace({'enero': '01', 'febrero': '02', 'marzo': '03', 'abril': '04', 'mayo': '05', 'junio': '06', 'julio': '07', 'agosto': '08', 'septiembre': '09', 'octubre': '10', 'noviembre': '11', 'diciembre': '12'}, regex=True)\n",
    "limpio_df['fecha'] = pd.to_datetime(limpio_df['fecha'], format='%d %m, %Y %I:%M %p')\n",
    "\n",
    "limpio_df['pregunta_norm'] = list(map(normalize_document, limpio_df['pregunta']))\n",
    "\n",
    "display(limpio_df)\n",
    "\n",
    "limpio_df.to_csv('./Base_limpia.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
